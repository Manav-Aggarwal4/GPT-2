# ML exploration!

In order to apply my ML knowledge, I built two ML models using PyTorchÂ ðŸ”¥:

1. I built a bigram model that predicts the next word based only on the previous one. I trained it using a dataset of hundreds of Yoda quotes (that I webscraped) to create a model that impersonates Yoda ðŸ˜Š. Since it's a bigram model, the performance isnâ€™t great, but I tuned the hyperparameters to make it as efficient as possible.

2. I'm currently working on the second model, which will have similar performance to OpenAI's GPT2!

